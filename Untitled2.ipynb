{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14d29df-73ae-4d4b-8ba1-ee1f110e8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of image files in 'F:/유형별 두피 이미지/Training': 168427\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images(directory, extensions=['.jpg', '.jpeg', '.png', '.bmp', '.ppm']):\n",
    "    count = 0\n",
    "    # 디렉토리를 순회하며 파일 검사\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # 파일 확장자가 지원되는 이미지 형식 중 하나와 일치할 경우 카운트\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = \"F:/유형별 두피 이미지/Training\"\n",
    "\n",
    "# 이미지 파일 수 계산\n",
    "total_images = count_images(base_dir)\n",
    "print(f\"Total number of image files in '{base_dir}': {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10441beb-6c54-4941-b7b0-165f538dc503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdirectories (Classes): ['[원천]모낭사이홍반_0.양호', '[원천]모낭사이홍반_1.경증', '[원천]모낭사이홍반_2.중등도', '[원천]모낭사이홍반_3.중증', '[원천]모낭홍반농포_0.양호', '[원천]모낭홍반농포_1.경증', '[원천]모낭홍반농포_2.중등도', '[원천]모낭홍반농포_3.중증', '[원천]미세각질_0.양호', '[원천]미세각질_1.경증', '[원천]미세각질_2.중등도', '[원천]미세각질_3.중증', '[원천]비듬_0.양호', '[원천]비듬_1', '[원천]비듬_2.중등도', '[원천]비듬_3.중증', '[원천]탈모_0.양호', '[원천]탈모_1.경증', '[원천]탈모_2.중등도', '[원천]탈모_3.중증', '[원천]피지과다_0.양호', '[원천]피지과다_1.경증', '[원천]피지과다_2.중등도', '[원천]피지과다_3.중증']\n",
      "[원천]모낭사이홍반_0.양호: 534 images\n",
      "[원천]모낭사이홍반_1.경증: 29960 images\n",
      "[원천]모낭사이홍반_2.중등도: 12957 images\n",
      "[원천]모낭사이홍반_3.중증: 4275 images\n",
      "[원천]모낭홍반농포_0.양호: 534 images\n",
      "[원천]모낭홍반농포_1.경증: 2126 images\n",
      "[원천]모낭홍반농포_2.중등도: 758 images\n",
      "[원천]모낭홍반농포_3.중증: 332 images\n",
      "[원천]미세각질_0.양호: 534 images\n",
      "[원천]미세각질_1.경증: 4435 images\n",
      "[원천]미세각질_2.중등도: 5486 images\n",
      "[원천]미세각질_3.중증: 2284 images\n",
      "[원천]비듬_0.양호: 534 images\n",
      "[원천]비듬_1: 16560 images\n",
      "[원천]비듬_2.중등도: 9523 images\n",
      "[원천]비듬_3.중증: 2256 images\n",
      "[원천]탈모_0.양호: 534 images\n",
      "[원천]탈모_1.경증: 13346 images\n",
      "[원천]탈모_2.중등도: 3797 images\n",
      "[원천]탈모_3.중증: 836 images\n",
      "[원천]피지과다_0.양호: 534 images\n",
      "[원천]피지과다_1.경증: 28061 images\n",
      "[원천]피지과다_2.중등도: 24485 images\n",
      "[원천]피지과다_3.중증: 3746 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_classes(directory):\n",
    "    class_files = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if dirs:\n",
    "            print(f\"Subdirectories (Classes): {dirs}\")\n",
    "        class_name = os.path.basename(root)\n",
    "        class_files[class_name] = []\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.ppm')):\n",
    "                class_files[class_name].append(file)\n",
    "        if class_files[class_name]:\n",
    "            print(f\"{class_name}: {len(class_files[class_name])} images\")\n",
    "\n",
    "base_dir = \"F:/유형별 두피 이미지/Training\"\n",
    "count_files_in_classes(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8b5ad3-b80d-453a-9909-f494fc34ed2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">61,488</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │        \u001b[38;5;34m61,488\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,234,037</span> (16.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,234,037\u001b[0m (16.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,488</span> (240.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,488\u001b[0m (240.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,978</span> (480.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m122,978\u001b[0m (480.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = 'efficientnet_model.keras'\n",
    "\n",
    "# 모델 로드\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 모델 요약 출력\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47b4c97-500e-4fb8-8d1a-8353b4da3bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134751 images belonging to 24 classes.\n",
      "Found 33676 images belonging to 24 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 145/2994\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:38:24\u001b[0m 5s/step - accuracy: 0.1933 - loss: 2.5264"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 데이터셋 위치 설정\n",
    "base_dir = \"F:/유형별 두피 이미지/Training\"\n",
    "\n",
    "# 데이터 생성기 설정\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20%를 검증 데이터로 사용\n",
    ")\n",
    "\n",
    "# 데이터 생성기 설정\n",
    "batch_size = 45  # 이 값은 예시입니다. 실제 계산값으로 조정 필요\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,  # 수정된 배치 사이즈\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,  # 수정된 배치 사이즈\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 콜백 설정\n",
    "model_checkpoint = ModelCheckpoint('model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,  # 에포크당 스텝 수 자동 계산\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=30,  # 에포크 수\n",
    "    callbacks=[model_checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "def debug_data_loading(directory):\n",
    "    # 클래스별 파일 수 출력\n",
    "    for class_folder in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        if os.path.isdir(class_path):  # 폴더인 경우에만 진행\n",
    "            file_count = len([name for name in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, name))])\n",
    "            print(f\"Class '{class_folder}': {file_count} files\")\n",
    "\n",
    "debug_data_loading(base_dir)\n",
    "\n",
    "# 모델을 TensorFlow Lite 형식으로 변환\n",
    "def convert_to_tflite(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "convert_to_tflite('model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3348db-caf8-4543-8f7e-2080a4940285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603257373184_5_RH.jpg, Predicted class index: 45, Probability: 0.2667\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603257373184_6_BH.jpg, Predicted class index: 45, Probability: 0.2653\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603341963457_2_TH.jpg, Predicted class index: 45, Probability: 0.2661\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603341963457_3_TH.jpg, Predicted class index: 45, Probability: 0.2677\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603342351290_2_TH.jpg, Predicted class index: 45, Probability: 0.2665\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603342351290_3_TH.jpg, Predicted class index: 45, Probability: 0.2668\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# 모델 파일 경로\n",
    "model_path = 'efficientnet_model.keras'  # 모델 경로를 정확히 설정해야 합니다.\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 테스트 이미지 디렉토리\n",
    "test_dir = \"F:/test\"\n",
    "\n",
    "# 이미지 크기 설정 (EfficientNetB0에 맞게 조정, 필요에 따라 조정하세요)\n",
    "image_size = (128, 128)  # 모델이 훈련된 이미지 사이즈에 맞춰서 조정\n",
    "\n",
    "def process_and_predict(image_path, model):\n",
    "    img = load_img(image_path, target_size=image_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    predicted_probability = np.max(prediction)\n",
    "\n",
    "    return predicted_class, predicted_probability\n",
    "\n",
    "# 테스트 이미지 목록 불러오기\n",
    "test_images = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 각 이미지에 대한 예측 수행 및 결과 출력\n",
    "for image_path in test_images:\n",
    "    predicted_class, predicted_probability = process_and_predict(image_path, model)\n",
    "    print(f\"Image: {os.path.basename(image_path)}, Predicted class index: {predicted_class}, Probability: {predicted_probability:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1ea01e-beb8-4cd7-89c3-43e2c154610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168427 images belonging to 24 classes.\n",
      "Class indices: {0: '[원천]모낭사이홍반_0.양호', 1: '[원천]모낭사이홍반_1.경증', 2: '[원천]모낭사이홍반_2.중등도', 3: '[원천]모낭사이홍반_3.중증', 4: '[원천]모낭홍반농포_0.양호', 5: '[원천]모낭홍반농포_1.경증', 6: '[원천]모낭홍반농포_2.중등도', 7: '[원천]모낭홍반농포_3.중증', 8: '[원천]미세각질_0.양호', 9: '[원천]미세각질_1.경증', 10: '[원천]미세각질_2.중등도', 11: '[원천]미세각질_3.중증', 12: '[원천]비듬_0.양호', 13: '[원천]비듬_1', 14: '[원천]비듬_2.중등도', 15: '[원천]비듬_3.중증', 16: '[원천]탈모_0.양호', 17: '[원천]탈모_1.경증', 18: '[원천]탈모_2.중등도', 19: '[원천]탈모_3.중증', 20: '[원천]피지과다_0.양호', 21: '[원천]피지과다_1.경증', 22: '[원천]피지과다_2.중등도', 23: '[원천]피지과다_3.중증'}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603257373184_5_RH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603257373184_6_BH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603341963457_2_TH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603341963457_3_TH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603342351290_2_TH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603342351290_3_TH.jpg, Predicted class: Index 45 not in class labels, Probability: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 훈련 데이터셋 위치 설정\n",
    "train_dir = \"F:/유형별 두피 이미지/Training\"\n",
    "\n",
    "# 데이터 생성기 설정\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 훈련 데이터 생성기\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 클래스 인덱스와 이름 매핑\n",
    "class_labels = train_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}  # 인덱스: 클래스 이름으로 변경\n",
    "\n",
    "# 클래스 레이블 매핑 확인\n",
    "print(\"Class indices:\", class_labels)\n",
    "\n",
    "# 예측 함수 수정\n",
    "def process_and_predict(image_path, model, class_labels):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "\n",
    "    # 클래스 레이블 매핑 검사\n",
    "    if predicted_class_index in class_labels:\n",
    "        predicted_class_name = class_labels[predicted_class_index]\n",
    "        predicted_probability = np.max(prediction)\n",
    "        return predicted_class_name, predicted_probability\n",
    "    else:\n",
    "        return f\"Index {predicted_class_index} not in class labels\", 0.0\n",
    "\n",
    "# 예측 수행\n",
    "for image_path in test_images:\n",
    "    predicted_class_name, predicted_probability = process_and_predict(image_path, model, class_labels)\n",
    "    print(f\"Image: {os.path.basename(image_path)}, Predicted class: {predicted_class_name}, Probability: {predicted_probability:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1798e77b-8075-4353-bc08-1d6c3155fea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">61,488</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)             │        \u001b[38;5;34m61,488\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,234,037</span> (16.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,234,037\u001b[0m (16.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,488</span> (240.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,488\u001b[0m (240.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,978</span> (480.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m122,978\u001b[0m (480.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b151874-7829-4aab-abae-a63a9b3b3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5130 images belonging to 9 classes.\n",
      "Found 5130 images belonging to 9 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 - 620s - 5s/step - accuracy: 0.3281 - loss: 1.6317 - val_accuracy: 0.1676 - val_loss: 2.5247 - learning_rate: 0.0010\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error type: AttributeError\n",
      "Error details: 'NoneType' object has no attribute 'items'\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\suong\\AppData\\Local\\Temp\\tmpi49fc9cq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\suong\\AppData\\Local\\Temp\\tmpi49fc9cq\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 데이터셋 위치 설정\n",
    "train_dir = \"F:/유형별 두피 이미지/Training\"\n",
    "validation_dir = \"F:/유형별 두피 이미지/validation1\"  # 검증 데이터 폴더 경로 수정\n",
    "\n",
    "# 데이터 생성기 설정\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen_validation = ImageDataGenerator(rescale=1./255)  # 검증 데이터셋은 데이터 증강 없이 rescale만 적용\n",
    "\n",
    "# 훈련 데이터 생성기\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=45,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 검증 데이터 생성기\n",
    "validation_generator = datagen_validation.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=45,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "output = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 콜백 설정\n",
    "model_checkpoint = ModelCheckpoint('model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "# 훈련 및 검증 데이터셋 크기와 배치 수 확인\n",
    "train_steps = math.ceil(train_generator.samples / train_generator.batch_size)\n",
    "validation_steps = math.ceil(validation_generator.samples / validation_generator.batch_size)\n",
    "\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        epochs=30,\n",
    "        callbacks=[model_checkpoint, early_stopping, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error type:\", type(e).__name__)\n",
    "    print(\"Error details:\", str(e))\n",
    "# 모델을 TensorFlow Lite 형식으로 변환\n",
    "def convert_to_tflite(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "convert_to_tflite('model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab4fae0-888f-47c4-856e-6c4877d893aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class [원천]모낭사이홍반_0.양호: 534 images\n",
      "Class [원천]모낭홍반농포_0.양호: 534 images\n",
      "Class [원천]모낭홍반농포_2.중등도: 758 images\n",
      "Class [원천]모낭홍반농포_3.중증: 332 images\n",
      "Class [원천]미세각질_0.양호: 534 images\n",
      "Class [원천]비듬_0.양호: 534 images\n",
      "Class [원천]탈모_0.양호: 534 images\n",
      "Class [원천]탈모_3.중증: 836 images\n",
      "Class [원천]피지과다_0.양호: 534 images\n",
      "Class [원천]모낭사이홍반_0.양호: 534 images\n",
      "Class [원천]모낭홍반농포_0.양호: 534 images\n",
      "Class [원천]모낭홍반농포_2.중등도: 758 images\n",
      "Class [원천]모낭홍반농포_3.중증: 332 images\n",
      "Class [원천]미세각질_0.양호: 534 images\n",
      "Class [원천]비듬_0.양호: 534 images\n",
      "Class [원천]탈모_0.양호: 534 images\n",
      "Class [원천]탈모_3.중증: 836 images\n",
      "Class [원천]피지과다_0.양호: 534 images\n",
      "Total training images: 5130, Total classes: 9\n",
      "Total validation images: 5130, Total classes: 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_and_classes(directory):\n",
    "    total_files = 0\n",
    "    classes = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    num_classes = len(classes)\n",
    "    for class_name in classes:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        file_count = len([name for name in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, name))])\n",
    "        total_files += file_count\n",
    "        print(f\"Class {class_name}: {file_count} images\")\n",
    "    \n",
    "    return total_files, num_classes\n",
    "\n",
    "train_dir = 'F:\\유형별 두피 이미지\\Training'\n",
    "validation_dir = 'F:\\유형별 두피 이미지\\Validation1'\n",
    "\n",
    "train_files, train_classes = count_files_and_classes(train_dir)\n",
    "validation_files, validation_classes = count_files_and_classes(validation_dir)\n",
    "\n",
    "print(f\"Total training images: {train_files}, Total classes: {train_classes}\")\n",
    "print(f\"Total validation images: {validation_files}, Total classes: {validation_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379d84a0-1c35-4a1b-9c57-d2ad58646a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet\n",
      "  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: scikit-image in f:\\anaconda3\\lib\\site-packages (from efficientnet) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in f:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
      "Requirement already satisfied: h5py in f:\\anaconda3\\lib\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.11.0)\n",
      "Requirement already satisfied: scipy>=1.8 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (1.11.4)\n",
      "Requirement already satisfied: networkx>=2.8 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (10.2.0)\n",
      "Requirement already satisfied: imageio>=2.27 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in f:\\anaconda3\\lib\\site-packages (from scikit-image->efficientnet) (0.3)\n",
      "Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
      "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.7/50.7 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: keras-applications, efficientnet\n",
      "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -U efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b1f0f6-abd4-4d63-9758-3b5f8e561d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5130 images belonging to 9 classes.\n",
      "Found 5130 images belonging to 9 classes.\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "\u001b[1m16804768/16804768\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 4s/step - accuracy: 0.2854 - loss: 1.8106 - val_accuracy: 0.1982 - val_loss: 3.0319\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/160\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:01\u001b[0m 3s/step - accuracy: 0.4375 - loss: 1.4416"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4375 - loss: 1.4416 - val_accuracy: 0.2000 - val_loss: 3.1511\n",
      "Epoch 3/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 3s/step - accuracy: 0.3615 - loss: 1.5370 - val_accuracy: 0.3104 - val_loss: 1.8378\n",
      "Epoch 4/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1562 - loss: 2.2894 - val_accuracy: 0.5000 - val_loss: 1.1306\n",
      "Epoch 5/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 3s/step - accuracy: 0.3762 - loss: 1.4570 - val_accuracy: 0.3516 - val_loss: 1.7630\n",
      "Epoch 6/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.3438 - loss: 1.5287 - val_accuracy: 0.2000 - val_loss: 2.3077\n",
      "Epoch 7/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 3s/step - accuracy: 0.3754 - loss: 1.4501 - val_accuracy: 0.3754 - val_loss: 1.5758\n",
      "Epoch 8/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.2500 - loss: 1.7207 - val_accuracy: 0.3000 - val_loss: 1.2090\n",
      "Epoch 9/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 3s/step - accuracy: 0.3757 - loss: 1.4443 - val_accuracy: 0.3939 - val_loss: 1.4388\n",
      "Epoch 10/10\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.3750 - loss: 1.5037 - val_accuracy: 0.4000 - val_loss: 1.1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 다음 경로에 저장되었습니다: F:\\유형별 두피 이미지\\model_efficientnet.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_dir = 'F:\\유형별 두피 이미지\\Training'\n",
    "validation_dir = 'F:\\유형별 두피 이미지\\Validation1'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_files // 32,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_files // 32\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = r'F:\\유형별 두피 이미지\\model_efficientnet.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"모델이 다음 경로에 저장되었습니다: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c51f151-2755-41f0-953b-5fbdf931a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Image: 0013_A2LEBJJDE00060O_1603942897337_6_BH.jpg - Predicted class: 증상: 미세각질, 중증도 (0): 양호\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1605242422804_6_BH.jpg - Predicted class: 증상: 미세각질, 중증도 (0): 양호\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1605242451450_2_TH.jpg - Predicted class: 증상: 미세각질, 중증도 (0): 양호\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Image: 0013_A2LEBJJDE00060O_1605534137037_6_BH.jpg - Predicted class: 증상: 미세각질, 중증도 (0): 양호\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 모델 로드\n",
    "model_path = 'F:\\유형별 두피 이미지\\model_efficientnet.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return tf.keras.applications.efficientnet.preprocess_input(img_array_expanded_dims)\n",
    "\n",
    "# 이미지 경로\n",
    "test_dir = r'F:\\test'\n",
    "image_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# 클래스 이름 (수동 설정 필요)\n",
    "class_labels = ['[원천]모낭사이홍반_0.양호', '[원천]모낭홍반농포_0.양호', '[원천]모낭홍반농포_2.중등도', '[원천]모낭홍반농포_3.중증', \n",
    "                '[원천]미세각질_0.양호', '[원천]비듬_0.양호', '[원천]탈모_0.양호', '[원천]탈모_3.중증', '[원천]피지과다_0.양호']\n",
    "\n",
    "# 증상과 중증도 추출 및 포맷팅 함수\n",
    "def format_class_name(class_name):\n",
    "    parts = class_name.split('_')\n",
    "    symptom = parts[0][4:]  # '원천' 문자 제거\n",
    "    severity = parts[1].split('.')[1]  # 중증도 번호와 설명 분리\n",
    "    return f\"증상: {symptom}, 중증도 ({parts[1][0]}): {severity}\"\n",
    "\n",
    "# 각 이미지에 대해 예측 수행\n",
    "for img_path in image_files:\n",
    "    processed_image = load_and_preprocess_image(img_path)\n",
    "    predictions = model.predict(processed_image)\n",
    "    predicted_class = class_labels[np.argmax(predictions)]\n",
    "    formatted_name = format_class_name(predicted_class)\n",
    "    print(f\"Image: {os.path.basename(img_path)} - Predicted class: {formatted_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79bad62a-332a-4331-9592-ce4f7c2c69d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5130 images belonging to 9 classes.\n",
      "Found 5130 images belonging to 9 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 3s/step - accuracy: 0.2850 - loss: 1.8073 - val_accuracy: 0.1495 - val_loss: 2.8087\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 56\u001b[0m\n\u001b[0;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     50\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     52\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 모델 훈련\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     57\u001b[0m     train_generator,\n\u001b[0;32m     58\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_generator),\n\u001b[0;32m     59\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     60\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[0;32m     61\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_generator)\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m     65\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m유형별 두피 이미지\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_efficientnet.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mF:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mF:\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:350\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    330\u001b[0m             x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    331\u001b[0m             y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    338\u001b[0m         )\n\u001b[0;32m    339\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    340\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    341\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m         _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    351\u001b[0m     }\n\u001b[0;32m    352\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m    354\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, epoch_logs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_dir = r'F:\\유형별 두피 이미지\\Training'\n",
    "validation_dir = r'F:\\유형별 두피 이미지\\Validation1'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = r'F:\\유형별 두피 이미지\\model_efficientnet.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"모델이 다음 경로에 저장되었습니다: {model_save_path}\")\n",
    "\n",
    "# 훈련 및 검증 정확도 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 훈련 및 검증 손실 시각화\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fa53aba-7357-4971-ab60-223112e6d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5130 images belonging to 9 classes.\n",
      "Found 5130 images belonging to 9 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 4s/step - accuracy: 0.2979 - loss: 1.7957 - val_accuracy: 0.1922 - val_loss: 3.1277\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(validation_generator\u001b[38;5;241m.\u001b[39mfilenames) \u001b[38;5;241m%\u001b[39m validation_generator\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m     61\u001b[0m     validation_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 63\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     64\u001b[0m     train_generator,\n\u001b[0;32m     65\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_steps,\n\u001b[0;32m     66\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     67\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[0;32m     68\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_steps\n\u001b[0;32m     69\u001b[0m )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m     72\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m유형별 두피 이미지\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_efficientnet.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mF:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mF:\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:350\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    330\u001b[0m             x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    331\u001b[0m             y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m             shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    338\u001b[0m         )\n\u001b[0;32m    339\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    340\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    341\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m         _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m     val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    351\u001b[0m     }\n\u001b[0;32m    352\u001b[0m     epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m    354\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, epoch_logs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_dir = r'F:\\유형별 두피 이미지\\Training'\n",
    "validation_dir = r'F:\\유형별 두피 이미지\\Validation1'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "train_steps = len(train_generator.filenames) // train_generator.batch_size\n",
    "validation_steps = len(validation_generator.filenames) // validation_generator.batch_size\n",
    "if len(train_generator.filenames) % train_generator.batch_size:\n",
    "    train_steps += 1\n",
    "if len(validation_generator.filenames) % validation_generator.batch_size:\n",
    "    validation_steps += 1\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = r'F:\\유형별 두피 이미지\\model_efficientnet.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"모델이 다음 경로에 저장되었습니다: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bada93-d2fa-476f-bf41-6029b3e9de59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
